{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 3: Aggregator\n",
        "======================\n",
        "\n",
        "In the previous tutorial, we fitted 3 _Datasets_ with an identical phase, outputting the results of each to a unique\n",
        "folder on our hard disk.\n",
        "\n",
        "However, lets use the _Aggregator_ to load the _Result_'s and manipulate / plot them using our Jupyter notebook. The API\n",
        "for using _Result_'s follow closely tutorial 1 of this chapter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from autoconf import conf\n",
        "import autofit as af\n",
        "from howtofit.chapter_2_results import src as htf\n",
        "\n",
        "import numpy as np\n",
        "from pyprojroot import here\n",
        "\n",
        "workspace_path = str(here())\n",
        "print(\"Workspace Path: \", workspace_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup the configs as we did in the previous tutorial, as well as the output folder for our non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/howtofit/config\",\n",
        "    output_path=f\"{workspace_path}/howtofit/output/chapter_2\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load the results of the previous tutorial into the _Aggregator_, we simply point the _Aggregator_ class to the path \n",
        "of the results we want it to load."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_path = f\"{workspace_path}/howtofit/output/chapter_2/aggregator\"\n",
        "\n",
        "agg = af.Aggregator(directory=str(output_path))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To begin, let me quickly explain what a generator is in Python, for those unaware. A generator is an object that \n",
        "iterates over a function when it is called. The _Aggregator_ creates all objects as generators, rather than lists, or \n",
        "dictionaries, or whatever.\n",
        "\n",
        "Why? Because lists and dictionaries store every entry in memory simultaneously. If you fit many _Dataset_'s, you'll \n",
        "have lots of results and therefore use a lot of memory. This will crash your laptop! On the other hand, a generator \n",
        "only stores the object in memory when it runs the function; it is free to overwrite it afterwards. Thus, your laptop \n",
        "won't crash!\n",
        "\n",
        "There are two things to bare in mind with generators:\n",
        "\n",
        " 1) A generator has no length, thus to determine how many entries of data it corresponds to you first must turn it to a \n",
        "    list.\n",
        "\n",
        " 2) Once we use a generator, we cannot use it again - we'll need to remake it. For this reason, we typically avoid \n",
        "    storing the generator as a variable and instead use the _Aggregator_ to create them on use.\n",
        "\n",
        "We can now create a _Samples_ generator of every fit. This creates instances of the _Samples_ class we manipulated in\n",
        "tutorial 1, which with the _Aggregator_ now acts as an interface between the results of the non-linear fit on your \n",
        "hard-disk and Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "samples_gen = agg.values(\"samples\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we print this list of outputs you should see over 3 different NestSamples instances, corresponding to the 3\n",
        "model-fits we performed in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"Emcee Samples:\\n\")\n",
        "print(samples_gen)\n",
        "print(\"Total Samples Objects = \", len(list(samples_gen)), \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We've encountered the _Samples_ class in previous tutorials. As we saw in tutorial 1, the Samples class contains all \n",
        "the accepted parameter samples of the non-linear search, which is a list of lists where:\n",
        "\n",
        " - The outer list is the size of the total number of samples.\n",
        " - The inner list is the size of the number of free parameters in the fit.\n",
        "\n",
        "With the _Aggregator_ we can now get information on the _Samples_ of all 3 model-fits, as opposed to just 1 fit using \n",
        "its _Result_ object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"All parameters of the very first sample\")\n",
        "    print(samples.parameters[0])\n",
        "    print(\"The tenth sample's third parameter\")\n",
        "    print(samples.parameters[9][2])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the _Aggregator_ to get information on the likelihoods, priors, weights, etc. of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for samples in agg.values(\"samples\"):\n",
        "    print(\"log(likelihood), log(prior), log(posterior) and weight of the tenth sample.\")\n",
        "    print(samples.log_likelihoods[9])\n",
        "    print(samples.log_priors[9])\n",
        "    print(samples.log_posteriors[9])\n",
        "    print(samples.weights[9])\n",
        "    print()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the _Sample_'s to create a list of the maximum log likelihood model of each fit to our three images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vector = [samps.max_log_likelihood_vector for samps in agg.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Parameter Lists:\\n\")\n",
        "print(vector, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As discussed in tutorial 1, using vectors isn't too much use, as we can't be sure which values correspond to which \n",
        "parameters.\n",
        "\n",
        "We can use the _Aggregator_ to create the maximum log likelihood model instance of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [samps.max_log_likelihood_instance for samps in agg.values(\"samples\")]\n",
        "print(\"Maximum Log Likelihood Model Instances:\\n\")\n",
        "print(instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model instance contains all the model components of our fit which for the fits above was a single gaussian\n",
        "profile (the word 'gaussian' comes from what we called it in the CollectionPriorModel when making the phase above)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].profiles.gaussian)\n",
        "print(instances[1].profiles.gaussian)\n",
        "print(instances[2].profiles.gaussian)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This, of course, gives us access to any individual parameter of our maximum log likelihood model. Below, we see that \n",
        "the 3 Gaussians were simulated using sigma values of 1.0, 5.0 and 10.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(instances[0].profiles.gaussian.sigma)\n",
        "print(instances[1].profiles.gaussian.sigma)\n",
        "print(instances[2].profiles.gaussian.sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also access the 'median pdf' model via the _Aggregator_, as we saw for the _Samples_ object in tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mp_vectors = [samps.median_pdf_vector for samps in agg.values(\"samples\")]\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "\n",
        "print(\"Median PDF Model Parameter Lists:\\n\")\n",
        "print(mp_vectors, \"\\n\")\n",
        "print(\"Most probable Model Instances:\\n\")\n",
        "print(mp_instances, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also print the \"model_results\" of all phases, which is string that summarizes every fit's model providing\n",
        "quick inspection of all results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "results = agg.model_results\n",
        "print(\"Model Results Summary:\\n\")\n",
        "print(results, \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets end the tutorial with something more ambitious. Lets create a plot of the inferred sigma values vs intensity of \n",
        "each _Gaussian_ profile, including error bars at 3 sigma confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mp_instances = [samps.median_pdf_instance for samps in agg.values(\"samples\")]\n",
        "ue3_instances = [\n",
        "    samp.error_instance_at_upper_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "le3_instances = [\n",
        "    samp.error_instance_at_lower_sigma(sigma=3.0) for samp in agg.values(\"samples\")\n",
        "]\n",
        "\n",
        "mp_sigmas = [instance.profiles.gaussian.sigma for instance in mp_instances]\n",
        "ue3_sigmas = [instance.profiles.gaussian.sigma for instance in ue3_instances]\n",
        "le3_sigmas = [instance.profiles.gaussian.sigma for instance in le3_instances]\n",
        "mp_intensitys = [instance.profiles.gaussian.sigma for instance in mp_instances]\n",
        "ue3_intensitys = [instance.profiles.gaussian.sigma for instance in ue3_instances]\n",
        "le3_intensitys = [instance.profiles.gaussian.intensity for instance in le3_instances]\n",
        "\n",
        "plt.errorbar(\n",
        "    x=mp_sigmas,\n",
        "    y=mp_intensitys,\n",
        "    marker=\".\",\n",
        "    linestyle=\"\",\n",
        "    xerr=[le3_sigmas, ue3_sigmas],\n",
        "    yerr=[le3_intensitys, ue3_intensitys],\n",
        ")\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
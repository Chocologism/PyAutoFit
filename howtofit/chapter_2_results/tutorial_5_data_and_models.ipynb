{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tutorial 5: Data and Models\n",
        "===========================\n",
        "\n",
        "Up to now, we've used used the _Aggregator_ to load and inspect the _Samples_ of 3 model-fits..\n",
        "\n",
        "In this tutorial, we'll look at how the way we designed our source code makes it easy to use the _Aggregator_ to\n",
        "inspect, interpret and plot the results of the model-fit, including refitting the best models to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from autoconf import conf\n",
        "import autofit as af\n",
        "from howtofit.chapter_2_results import src as htf\n",
        "\n",
        "import os\n",
        "\n",
        "workspace_path = os.environ[\"WORKSPACE\"]\n",
        "print(\"Workspace Path: \", workspace_path)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup the configs as we did in the previous tutorial, as well as the output folder for our non-linear search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conf.instance = conf.Config(\n",
        "    config_path=f\"{workspace_path}/howtofit/config\",\n",
        "    output_path=f\"{workspace_path}/howtofit/output/chapter_2\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load these results with the _Aggregator_, we again point it to the path of the results we want it to inspect, with\n",
        "our path straight to the _Aggregator_ results ensuring w don't need to filter our _Aggregator_ in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "output_path = f\"{workspace_path}/howtofit/output/chapter_2/aggregator\"\n",
        "agg = af.Aggregator(directory=str(output_path))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the _Aggregator_ to load a generator of every fit's dataset, by changing the 'output' attribute to the \n",
        "'dataset' attribute at the end of the aggregator.\n",
        "\n",
        "Note that in the source code for chapter 2, specifically in the 'phase.py' module, we specified that the the _Dataset_ \n",
        "object would be saved too hard-disk such that the _Aggregator_ can load it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg.values(\"dataset\")\n",
        "print(\"Datasets:\")\n",
        "print(list(dataset_gen), \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is here the object-oriented design of our plot module comes into its own. We have the _Dataset_ objects loaded, \n",
        "meaning we can easily plot each _Dataset_ using the 'dataset_plot.py' module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in agg.values(\"dataset\"):\n",
        "    htf.plot.Dataset.data(dataset=dataset)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The _Dataset_ names are available, either as part of the _Dataset_ or via the aggregator's dataset_names method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for dataset in agg.values(\"dataset\"):\n",
        "    print(dataset.name)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The info dictionary we input into the pipeline is also available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for info in agg.values(\"info\"):\n",
        "    print(info)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can repeat the same trick to get the mask of every fit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mask_gen = agg.values(\"mask\")\n",
        "print(\"Masks:\")\n",
        "print(list(mask_gen), \"\\n\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to refit each dataset with the maximum log likelihood model-fit of each phase. To do this, we'll need \n",
        "each phase's masked dataset.\n",
        "\n",
        "(If you are unsure what the 'zip' is doing below, it essentially combines the'datasets' and 'masks' lists in such\n",
        "a way that we can iterate over the two simultaneously to create each MaskedDataset).\n",
        "\n",
        "The _MaskedDataset_ may have been altered by the *data_trim_left* and *data_trim_right* custom phase settings. We can \n",
        "load the _SettingsPhase_ via the _Aggregator_ to use these settings when we create the _MaskedDataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset_gen = agg.values(\"dataset\")\n",
        "mask_gen = agg.values(\"mask\")\n",
        "settings_gen = agg.values(\"settings\")\n",
        "\n",
        "masked_datasets = [\n",
        "    htf.MaskedDataset(\n",
        "        dataset=dataset, mask=mask, settings=settings.settings_masked_dataset\n",
        "    )\n",
        "    for dataset, mask, settings in zip(dataset_gen, mask_gen, settings_gen)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a problem with how we set up the _MaskedDataset_'s above, can you guess what it is?\n",
        "\n",
        "We used lists! If we had fit a large sample of data, the above object would store the _MaskedDataset_ of all objects\n",
        "simultaneously in memory on our hard-disk, likely crashing our laptop! To avoid this, we must write functions that\n",
        "manipulate the _Aggregator_ generators as generators themselves. Below is an example function that performs the same\n",
        "task as above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def masked_dataset_from_agg_obj(agg_obj):\n",
        "\n",
        "    dataset = agg_obj.dataset\n",
        "    mask = agg_obj.mask\n",
        "    settings = agg_obj.settings\n",
        "\n",
        "    return htf.MaskedDataset(\n",
        "        dataset=dataset, mask=mask, settings=settings.settings_masked_dataset\n",
        "    )\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To manipulate this function as a generator using the _Aggregator_, we apply it to the _Aggregator_'s map function.\n",
        "\n",
        "The *masked_dataset_gen* below ensures that we avoid representing all _MaskeddDataset_'s simultaneously in memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "masked_dataset_gen = agg.map(func=masked_dataset_from_agg_obj)\n",
        "print(list(masked_dataset_gen))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets get the the maximum likelihood model instances, as we did in tutorial 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "instances = [samps.max_log_likelihood_instance for samps in agg.values(\"samples\")]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Okay, we want to inspect the fit of each maximum log likelihood model. To do this, we reperform each fit.\n",
        "\n",
        "First, we need to create the model-data of every maximum log likelihood model instance. Lets begin by creating a list \n",
        "of profiles of every phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "profiles = [instance.profiles for instance in instances]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use these to create the model data of each set of profiles (Which in this case is just 1 Gaussian, but had\n",
        "we included more profiles in the model would consist of multiple _Gaussian_'s / _Exponential_'s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model_datas = [\n",
        "    profile.gaussian.profile_from_xvalues(xvalues=dataset.xvalues)\n",
        "    for profile, dataset in zip(profiles, agg.values(\"dataset\"))\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, as we did in tutorial 2, we can combine the _MaskedDataset_'s and model_datas in a _Fit_ object to create the\n",
        "maximum likelihood fit of each phase!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fits = [\n",
        "    htf.FitDataset(masked_dataset=masked_dataset, model_data=model_data)\n",
        "    for masked_dataset, model_data in zip(masked_datasets, model_datas)\n",
        "]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now plot different components of the fit (again benefiting from how we set up the 'fit_plots.py' module)!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for fit in fits:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again, the code above does not use generators and could prove memory intensive for large datasets. Below is how we \n",
        "would perform the above task with generator functions, using the *masked_dataset_gen* above for the _MaskedDataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def model_data_from_agg_obj(agg_obj):\n",
        "\n",
        "    xvalues = agg_obj.dataset.xvalues\n",
        "    instance = agg_obj.samples.max_log_likelihood_instance\n",
        "    profiles = instance.profiles\n",
        "\n",
        "    return sum([profile.profile_from_xvalues(xvalues=xvalues) for profile in profiles])\n",
        "\n",
        "\n",
        "def fit_from_agg_obj(agg_obj):\n",
        "\n",
        "    masked_dataset = masked_dataset_from_agg_obj(agg_obj=agg_obj)\n",
        "    model_data = model_data_from_agg_obj(agg_obj=agg_obj)\n",
        "\n",
        "    return htf.FitDataset(masked_dataset=masked_dataset, model_data=model_data)\n",
        "\n",
        "\n",
        "fit_gen = agg.map(func=fit_from_agg_obj)\n",
        "\n",
        "for fit in fit_gen:\n",
        "    htf.plot.FitDataset.residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.normalized_residual_map(fit=fit)\n",
        "    htf.plot.FitDataset.chi_squared_map(fit=fit)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setting up the above objects (the masked_datasets, model datas, fits) was a bit of work. It wasn't too many lines of \n",
        "code, but for something our users will want to do many times it'd be nice to have a short cut to setting them up, right?\n",
        "\n",
        "In the source code module 'aggregator.py' we've set up exactly such a short-cut. This module simply contains the \n",
        "generator functions above such that the generator can be created by passing the _Aggregator_. This provides us with \n",
        "convenience methods for quickly creating the _MaskedDataset_, model data and _Fit_'s using a single line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "masked_dataset_gen = htf.agg.masked_dataset_generator_from_aggregator(aggregator=agg)\n",
        "model_data_gen = htf.agg.model_data_generator_from_aggregator(aggregator=agg)\n",
        "fit_gen = htf.agg.fit_generator_from_aggregator(aggregator=agg)\n",
        "\n",
        "htf.plot.FitDataset.residual_map(fit=list(fit_gen)[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The methods in 'aggregator.py' actually allow us to go one step further: they all us to create the _MaskedDataset_ and\n",
        "_Fit_ objects using an input _SettingsMaskedDataset_. This means we can fit a _Dataset_ with a _Phase_ and then see how\n",
        "the model-fits change if we set the _Dataset_ in different ways.\n",
        "\n",
        "Below, we create and plot a _Fit_ where the _MaskedDataset_ is trimmed from the left and right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "settings_masked_dataset = htf.SettingsMaskedDataset(\n",
        "    data_trim_left=20, data_trim_right=20\n",
        ")\n",
        "\n",
        "fit_gen = htf.agg.fit_generator_from_aggregator(\n",
        "    aggregator=agg, settings_masked_dataset=settings_masked_dataset\n",
        ")\n",
        "\n",
        "htf.plot.FitDataset.residual_map(fit=list(fit_gen)[0])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For your model-fitting project, you'll need to update the 'aggregator.py' module in the same way. This is why we have \n",
        "emphasised the object-oriented design of our model-fitting project through. This design makes it very easy to inspect \n",
        "results via the _Aggregator_ later on!"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}